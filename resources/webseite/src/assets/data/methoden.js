const questions = [
    {
        id: 1,
        title: "Historisches Bildmaterial",
        info: "<h5>Dokumentation der Gruppe \"Historisches Bildmaterial</h5>" +
            "<p>Das historische Bildmaterial dient dazu, die städtebaulichen und gesellschaftlichen Veränderungen in Trier im 20. Jahrhundert sichtbar zu machen. Bei der Auswahl der Bilder wurde besonderer Wert daraufgelegt, markante Gebäude und zentrale Orte von Trier (Placesoints of Interest, POIs) zu identifizieren, die den Wandel der Stadt exemplarisch veranschaulichen und einen besonderen Eindruck hinterließenassen. Ziel dieser digitalen Ausstellung ist es, die Bilder soweit wie möglich unter den Prinzipien von Open Access verfügbar zu machen, unter Berücksichtigung der rechtlichen Rahmenbedingungen. Darüber hinaus sind viele Bilder in der Karte wiederzufinden. \n" +
            "Vorgehen und Methoden</p>" +
            "<p>Für die Sammlung des Materials wurden verschiedene Quellen herangezogen, darunter insbesondere frei zugängliche Online-Sammlungen wie Wikimedia Commons. Um die Recherche nach spezifischen Zeiträumen zu erleichtern, wurde ein eigens entwickeltes Python-Skript eingesetzt. Neben Online-Recherchen wurden auch direkte Kontakte zu Institutionen genutzt. Besonders hilfreich erwiesen sich die Zusammenarbeit mit dem Volksfreund, dem Kulturzentrum TUFA und dem Landesarchiv Baden-Württemberg, die uns bereitwillig und effizient unterstützten.</p>" +
            "<p>Die gesammelten Bilder wurden sorgfältig geprüft, um ihre Qualität, Relevanz und Nachnutzbarkeit zu gewährleisten. Einige Bilder wurden mit Wasserzeichen versehen, um die Rechte direkt nachvollziehbar zu machen. Detaillierte Informationen, einschließlich Angaben zu den Creative-Commons-Lizenzen (CC) sind in den Metadaten dokumentiert. Für die Erstellung von Vorher-Nachher-Bildern kam das frei verfügbare Open-Source-Tool JuxtaposeJS. zum Einsatz. Die „Nachher-Bilder“ wurden von uns selbst aufgenommen. Die finale Bildauswahl umfasst überwiegend Fotografien sowie einige digitale Postkarten.</p>" +
            "<p>Während der Arbeit traten verschiedene Herausforderungen auf, insbesondere im Bereich der rechtlichen Klärung der Nachnutzungsrechte. Dabei ging es vor allem um:<p>" +
            "<ul class='punkt'>" +
            "<li><strong>Urheberrechtsfragen bei historischen Bildern:</strong> Nach deutschem Urheberrecht (§ 2 Abs. 2 UrhG) kann ein Urheber nur eine natürliche Person sein. Daher wird die US Army, die bei manchen Bildern als Urheber angegeben ist, in Deutschland nicht als solche anerkannt.</li>" +
            "<li><strong>Fristen für die Schutzdauer:</strong> Gemäß § 64 UrhG erlischt das Urheberrecht 70 Jahre nach dem Tod des Urhebers. Ist der Urheber unbekannt, gilt laut § 66 UrhG eine Frist von 70 Jahren ab der Veröffentlichung. Diese Regelung wurde für unbekannte Urheber der Bilder angewandt.</li>" +
            "<li>§ 64 UrhG: <a href='https://www.gesetze-im-internet.de/urhg/__64.html' target='_blank' class='link-info'>Allgemeines zur Schutzdauer</a></li>" +
            "<li>§ 66 UrhG: <a href='https://www.gesetze-im-internet.de/urhg/__66.html' target='_blank' class='link-info'>Anonyme und pseudonyme Werke</a></li>" +
            "<li>§ 2 UrhG: <a href='https://www.gesetze-im-internet.de/urhg/__2.html' target='_blank' class='link-info'>Definition geschützter Werke</a></li>" +
            "</ul>" +
            "<p>Eine weitere Schwierigkeit bestand in der Zugänglichkeit des Materials. Während einige Archive ihre Bestände digital bereitgestellt haben, waren bei anderen Institutionen spezielle Anfragen erforderlich, was zu Verzögerungen führte. Darüber hinaus wiesen die zur Verfügung gestellten Bildquellen teils unterschiedliche Metadatenformate und Bildauflösungen auf, was eine einheitliche Verarbeitung erschwerte.</p>" +
            "<h5>Lösungsansätze und Ergebnisse</h5>" +
            "<p>Diese Herausforderungen wurden durch intensive Kommunikation mit den Institutionen sowie die Fokussierung auf kostenfreie, digital verfügbare Materialien erfolgreich bewältigt. Die erstellte Sammlung wurde sowohl somit auch mit der digitalen Karte verknüpft und verfügbar gemacht. als auch mit der Diskursanalyse verknüpft. Ziel war es schlussendlich, möglichst viele Bilder frei zugänglich zu machen. Dieses Ziel konnte größtenteils erreicht werden. Für die Nachnutzung der Bilder wird jedoch empfohlen, die jeweiligen Lizenzen und Hinweise in den Metadaten zu berücksichtigen.</p>",
    },
    {
        id: 2,
        title: "Entwicklung in Karten",
        info: "<p>Durch die Darstellung der historischen Karten, die man über eine Zeitleiste und einen Transparenzregler anpassen kann, kann man die bauliche Entwicklung der Stadt nachvollziehen. Das wird durch Kontextualisierung und Einfügen weiterer Verknüpfungen (historische Fotos, Routen) unterstützt.</p>" +
            "<p>Dabei haben wir die Karten zunächst mit QGIS georeferenziert. Das heißt, die Digitalisate wurden über eine geografisch korrekte moderne Karte - in unserem Fall OpenStreetMap - gelegt und anhand von konkreten Punkten (z. B. Porta Nigra) an die moderne Karte angepasst. Dabei wurde die historische Karte entsprechend verzerrt und gedreht, sodass geografische Unstimmigkeiten ausgeglichen wurden.</p>" +
            "<p>Die Trierer Routen 2024 wurden durch Erstellung eines Shapedatei-Layers eingezeichnet. Dabei handelt es sich um eine Ebene über der OpenStreetMap, auf welcher geografische Informationen gespeichert werden können. Auf einer solchen Ebene wurden auch die Punkte für das Popup historischer Fotos gesetzt. Anschließend wurden die georeferenzierten Karten sowie die Layer mittels der Erweiterung qgis2web als OpenLayers und Leaflet für die Webseite exportiert. Für die Webdarstellung der Karten samt Metadaten und Kontextinformationen entwickelten wir ein Konzept. Nach dem Abschluss der technischen Arbeiten wurden die Karten mittels Quellen und Literatur kontextualisiert.</p>",
    },
    {
        id: 3,
        title: "Politischer Diskurs",
        info: "<h5>Textkorpus</h5>" +
            "<p>Die Grundlage für die Analyse des politischen Diskurses in Trier bot die städtische Rathaus-Zeitung. Innerhalb der seit 2006 wöchentlich am Dienstag erscheinenden Zeitung bekommen die Stadtratsfraktionen in der Rubrik „Meinung der Fraktionen“ die Möglichkeit sich an die Bewohner*innen Triers zu richten und ihre Positionen zu verschiedensten stadtpolitischen Themen zu äußern. Die Stadt Trier stellt die Inhalte der Rathauszeitung zurückgehend bis 2006 auf der eigenen Webseite zur Verfügung.</p>" +
            "<p>Die Texte der Parteien wurden mit Hilfe eines Python-Skripts und der Python-Library BeautifulSoup automatisiert abgerufen und als Text-Dateien abgespeichert. Die Textsammlung, auch Korpus genannt, enthält die Meinungen der Fraktionen von 2006 bis November 2024 und enthält insgesamt 751 Text-Dateien, in dem jeweils die Texte aller Parteien für ein Erscheinungsdatum der Rathauszeitung vorhanden sind. Anschließend wurde der vorliegende Korpus weiterverarbeitet und die Dateien in die Parteien unterteilt, womit letztlich ein Textkorpus mit insgesamt 4642 Text-Dateien für die weitere Analyse entstand.</p>" +
            "<p>Die Anzahl und die Jahreszahlen des Erscheinens der Texte pro Partei variiert jeweils je nachdem, ob die Partei im gesamten Betrachtungszeitraum im Stadtrat war und, ob sie ihre Meinung in der Rathauszeitung mitgeteilt hat. Die Parteien UBM, FWG und UBT wurden in der Untersuchung zusammen betrachtet, da es sich jeweils um Umbenennung desselben Bündnisses handelt.</p>" +
            "<p>Während eine qualitative Analyse, also das Lesen und Auswerten einzelner Texte, durch die große Menge an Dokumenten kaum möglich ist, gibt es verschiedene quantitative Ansätze, mit denen die Analyse großer Textmengen möglich gemacht wird. Durch die Verarbeitung vieler Texte lassen sich Trends und Muster erkennen, die bei der Betrachtung einzelner Texte kaum erkennbar sind. In diesem Zusammenhang wird auch von Distant Reading gesprochen.</p>" +
            "<p>Mit Hilfe des erstellen Textkorpus wurden nun die Fragen folgenden Fragen untersucht:</p>" +
            "<ul class=''>" +
            "<li><strong>Was?</strong> Über welche Orte Themen wird diskutiert?</li>" +
            "<li><strong>Wie?</strong> Mit welchen Emotionen wird diskutiert?</li>" +
            "<li><strong>Wo?</strong> Über welche Orte wird diskutiert?</li>" +
            "</ul>",
        sub: [{
            id: 6,
            title: "Topic Modeling",
            info: "<p>Welche Themen werden von den Fraktionen in der Rathauszeitung präsentiert und so als für die Entwicklung Triers relevant gesetzt?</p>" +
                "<p>Zur Annäherung an diese Frage wurde ein Topic Modeling durchgeführt." +
                "Dabei handelt es sich um eine Methode zur automatischen Themenerkennung, durch die mithilfe von statistischen und KI-gestützten Verfahren große Textsammlungen exploriert werden können. Ziel ist es dabei, die den Texten zugrundeliegenden Themen oder „Topics“ zu extrahieren, ohne dass diese im Vorfeld definiert werden müssen.</p>" +
                "<p>Ein Topic ist dabei eine Gruppe von Wörtern, die semantisch miteinander verbunden sind und häufig in ähnlichen Kontexten vorkommen - zum Beispiel die Wörter „ÖPNV“, „Bus“ und „Ticket“ im Bereich des öffentlichen Nahverkehrs. Obwohl ein Topic also auf maschinell ermittelten statistischen und semantischen Aspekten beruht und damit nicht exakt das Gleiche wie ein explizit inhaltlich definiertes Thema ist, stellt Topic Modeling eine effektive Methode dar, um relevante Themen in einer Textsammlung zu ermitteln.</p>" +
                "<p>Für die Analyse wurde BERTopic verwendet, ein auf Bidirectional Encoder Representations from Transformers (BERT) basierendes Verfahren. \n" +
                "Im Vergleich zu traditionelleren Verfahren zum Topic Modeling wie Latent Dirichlet Allocation (LDA) ist BERTopic dazu in der Lage, nicht nur statistische Zusammenhänge, sondern auch semantische Beziehungen zwischen Wörtern und den Kontext, in dem sie auftreten, zu berücksichtigen, was eine präziseren Themenanalyse der hier untersuchten Texte ermöglicht hat. BERTopic eignet sich besonders für die Analyse kurzer Texte, wie sie im Fall der Texte der Rathauszeitungsrubrik „Meinung der Fraktionen“ vorliegen. Darüber hinaus lässt sich BERTopic gut an spezifische Anforderungen anpassen, ermöglicht eine präzise Steuerung der Topicanzahl und bietet anschauliche Visualisierungsmöglichkeiten, die die Interpretation und Präsentation der Ergebnisse erleichterten.</p>" +
                "<p>Eine Orientierung über die thematische Struktur der Texte gibt die Intertopic Distance Map, welche die Häufigkeit verschiedener Topics sowie ihre thematische Verwandtschaft untereinander visualisiert. Um den inhaltlichen Kern jedes einzelnen Topics darzustellen, wurden Topic Word Scores verwendet, welche für jedes Topic die wichtigsten Wörter zeigen, die es charakterisieren. Die Labels der Topics wurden dabei händisch nach Durchsicht der Topic Word Scores sowie der entsprechenden Texte vergeben. Zusätzlich wurden die Verteilungen der Topicfrequenzen über die verschiedenen Parteien sowie über die Zeit hinweg visualisiert, um den Nutzer*innen die Exploration etwaiger Muster und Trends zu ermöglichen. Da die “Freien Wähler” und “Die Fraktion” nur mit sehr wenigen Beiträgen in der Rathauszeitung vertreten waren, wurden sie aus der Analyse der Topics nach Parteien ausgeschlossen.</p>" +
                "<p>Die einzelnen Arbeitsschritte und Parametereinstellungen können in Detail im entsprechenden Python-Skript auf unseren Github eingesehen werden.</p>",
        }, {
            id: 7,
            title: "Emotionsanalyse",
            info: "<p>Für die Emotionsanalyse der politischen Texte wurde das speziell an die deutsche Sprache angepasste RoBERTa-Modell <a href='https://huggingface.co/visegradmedia-emotion/Emotion_RoBERTa_german6_v7' class='link-info' target='_blank'>Emotion_RoBERTa_german6_v7</a> eingesetzt. Dieses Modell wurde entwickelt, um die dominierende Emotion in einem gegebenen Text zu identifizieren und unterscheidet zwischen sechs Emotionskategorien: “Wut”, “Angst”, “Ekel”, “Traurigkeit”, “Freude” und einer Kategorie \"Keine davon\".</p>" +
                "<p>Das Modell basiert auf der [RoBERTa-Architektur](https://huggingface.co/FacebookAI/roberta-base), die ursprünglich für die englische Sprache konzipiert wurde und sich besonders für die Klassifikation ganzer Sätze eignet. Die Anpassung an die deutsche Sprache erfolgte durch ein umfangreiches Fine-Tuning mit einem großen Korpus deutscher Texte, die manuell mit Emotionslabels versehen wurden.<p>" +
                "<p>In Bezug auf die Modellleistung erreichte \"Emotion_RoBERTa_german6_v7\" eine Gesamtgenauigkeit von 78% bei der Emotionsklassifikation auf einem unabhängigen Testdatensatz. Die F1-Scores für die einzelnen Emotionskategorien variierten zwischen 0,72 für \"Ekel\" und 0,85 für \"Freude\", was eine solide Leistung über alle Kategorien hinweg bedeutet.</p>" +
                "<p>Ziel der Emotionsanalyse der Parteitexte ist es, die Emotionen sichtbar zu machen, die die Parteien in ihren Texten der Trierer Rathauszeitung veröffentlichen. Diese Analyse bietet eine Grundlage für weiterführende Untersuchungen, z.B. in den Bereichen der Kommunikations- und Politikwissenschaften. Sie ermöglicht es Forschenden, die emotionalen Kommunikationsmuster politischer Parteien zu untersuchen und zu vergleichen.</p>" +
                "<p>Programmiert wurde eine abschnittsweise Analyse der Parteitexte, da davon ausgegangen wurde, dass ein Text mehrere Emotionen übermitteln kann. Der Absatz wurde als Trenneinheit gewählt, da diese häufig verwendet werden, um ein Thema abzuschließen und ein neues zu beginnen. Da das RoBERTa-Modell nur auf Analysen mit einer Länge von maximal 512 Zeichen reduziert ist, wurde bei längeren Abschnitten die Sliding-Window-Methode verwendet. Die Sliding-Window-Methode teilt lange Texte in überlappende Abschnitte auf, analysiert jeden Abschnitt separat und kombiniert die Ergebnisse, um eine Gesamtbewertung zu erhalten. Die Ergebnisse der Analyse wurden in einer CSV-Datei für die Erstellung der Grafiken, einer SVG-Datei mit der Häufigkeit der Scores ([siehe Abbildung 2]()) und in einer PKL-Datei für die weitere Verarbeitung (Lime-Analyse im zweiten Teil des Codes) gespeichert.</p>" +
                "<p>Die Anzahl der Texte pro Partei ist in <a href='https://trier-digital.github.io/#/sentiment' class='link-info'>Abbildung 1</a> dargestellt. Die Textanzahl der Parteien “CDU”, “SPD”, “FDP”, “Die Grünen” und “UBT/FWG/UBM” ist mit jeweils etwa 700 Texten relativ ausgeglichen. Anders als bei den Texten der Parteien - die seit 2006 veröffentlicht werden - wurden von der Partei “Die Linke” erst seit 2009 und bei der Partei “AFD” erst seit 2014 Texte veröffentlicht, was den Unterschied in der Textanzahl erklärt. In der Analyse wurden die Beiträge der Parteien “Freie Wähler” (11) und “Die Fraktion” (3) ausgeschlossen, da es nur sehr wenige Beiträge von ihnen gab.</p>" +
                "<p>Die in <a href='https://trier-digital.github.io/#/sentiment' class='link-info'>Abbildung 2</a> dargestellten Scores zeigen an, wie oft und mit welcher Sicherheit das Modell eine bestimmte Emotion ausgewählt hat. Die Analyse der Emotionsklassifikation ergab einen durchschnittlichen Konfidenzwert von 0,9289 mit einem Median von 0,9945 und einer Standardabweichung von 0,1335. Diese Werte weisen auf eine hohe Zuverlässigkeit und Konsistenz des verwendeten Klassifikationsmodells hin. Der hohe Durchschnittswert nahe 1 zeigt, dass das Modell in den meisten Fällen mit großer Sicherheit Vorhersagen trifft. Der hohe Median von 0,9945 zeigt, dass mehr als die Hälfte aller Klassifikationen mit einer Konfidenz von über 99 % erfolgte, was auf eine besonders ausgeprägte Entscheidungssicherheit des Modells hindeutet.</p>" +
                "<p>Die relativ geringe Standardabweichung von 0,1335 spricht ebenfalls für die Stabilität der Vorhersagen über verschiedene Texte hinweg. Dennoch weist die Präsenz einer Standardabweichung darauf hin, dass das Modell in einigen Fällen mit geringerer Sicherheit klassifiziert, was möglicherweise auf komplexere oder mehrdeutige Textpassagen zurückzuführen ist.<p>" +
                "<p>In einem zweiten Schritt wurde mithilfe der [Python-Bibliothek Lime](https://pypi.org/project/lime/) untersucht, welche Wörter für die Klassifikation der Absätze ausschlaggebend waren. Die Ergebnisse wurden in Wordclouds dargestellt.</p>" +
                "<p>Zur visuellen Darstellung der Ergebnisse wurden Wordclouds generiert, eine Technik, die die Häufigkeit und Bedeutung von Wörtern durch deren Größe und Positionierung in einer wolkenartigen Anordnung repräsentiert. Für jede Kombination aus Partei und Emotion wurde eine separate Wordcloud erstellt. Die Größe eines Wortes in der Wordcloud steht in direktem Zusammenhang mit seiner durch LIME ermittelten Bedeutung für die Klassifikation in die Emotionskategorie. Die Wordclouds ermöglichen eine intuitive und schnelle Erfassung der charakteristischen Sprache, die von verschiedenen politischen Parteien in Verbindung mit spezifischen Emotionen verwendet wird.</p>"
        },{
            id: 8,
            title: "Textanalyse",
            info: "<h5>Textanalyse</h5>" +
                "<p>Für die Analyse der Texte aus dem Korpus der Rathauszeitungen wurden verschiedene Methoden des Natural Language Processings (NLP) genutzt. Für die Beantwortung der Frage, über welche Orte in Trier besonders häufig diskutiert wurde, wurde zunächst die Python Library SpaCy genutzt, mit der eine Named Entity Recognition (NER) , also die computergestützte Erkennung von Eigennamen, durchgeführt wurde. Hierzu wurden Eigennamen des Typs „Location“ gewählt. Orte, die so erkannt wurden waren z.B. die Mosel oder auch die Stadt Trier. Da auch kleinere Örtlichkeiten, die möglicherweise eher eine lokale Bekanntheit erfahren, mit in die Analyse einbezogen werden sollten, wurden die Orte der NER mit einer Liste von Orten, in eigener Recherche entstand, zusammengefügt (à POI-Liste).</p>" +
                "<p>Für die weitere Untersuchung des Vorkommens der POIs, also der Placesoints of Interest, wurde mit Hilfe der POI-Liste und den Texten der Parteien eine Term-Document-Matrix (TDM) erstellt, aus der herausgelesen werden kann, welche POIs in welchen Textdateien vorkommt und, wenn sie vorkommen, wie häufig. Aus dieser Matrix kann unter anderem extrahiert werden, welche Worte am häufigsten genannt wurden und, wie die Verteilung der Worthäufigkeiten über die Jahre gelagert ist. Während sich für die weitere Untersuchung auf drei Orte fokussiert wurde, bietet die erstellte TDM die Möglichkeit der Nachnutzung, auch durch andere Forschungsinteressierte.</p>" +
                "<h5> Informationsvisualisierung</h5>" +
                "<p>Für die Visualisierung der aus dem Textkorpus gewonnenen Informationen wurde auf Wortwolken zurückgegriffen. Mit diesen lassen sich Worthäufigkeiten besonders gut und visuell ansprechend gestalten. Wörter, die besonders häufig im Text vorkommen, werden, hierbei größer und dicker dargestellt als Wörter, die eher weniger im Text vorkommen. Die Anzahl der Worte in der Wortwolke kann selbst bestimmt werden, wodurch entgegengewirkt werden kann, dass sehr selten vorkommende Worte angezeigt werden. Zusätzlich wurden für die Visualisierungen der POIs die Stoppwörter aus den Texten entfernt. Wortwolken bieten eine Möglichkeit sich über Themen und Texte einen Überblick zu verschaffen und in dem Fall der POIs die unterschiedliche Wortwahl und möglicherweise auch Schwerpunkte der Parteien zu verschiedenen Themen zu untersuchen.</p>"
        }]
    },
    {
        id: 4,
        title: "Webseite",
        info: "<p>Für das Erstellen der Webseite wurde das Framework Vue.js gewählt. Dieses erlaubt im Vergleich zur Verwendung von bloßem HTML eine wirtschaftlichere Verwendung von Ressourcen und Code-Stücken und erleichtert die Strukturierung des Projekts. Vue.js ermöglicht die Wiederverwendung einzelner Code-Komponenten, wie Bilder-Karussells oder Kacheln, auf mehreren Seiten. Auch das dynamische Erzeugen von Unterseiten für jedes Bild der Galerie wurde so ermöglicht. Metadaten für Fotos, Karten und SVG-Dateien wurden als JSON-Objekte gespeichert und können so dynamisch auf verschiedenen Seiten genutzt werden.</p>"+
            "<p>Das Design der Webseite basiert auf einem Bootstrap-Template, um ein einheitliches Aussehen zu garantieren, wurde aber den Anforderungen entsprechend angepasst. Die Farben der Webseite orientieren sich an einer Palette bestehend aus Blautönen, sowie verschiedenen Braun- und Orangetönen, die die Gebäudefarben des Trierer Hauptmarkts widerspiegeln. Für eine intuitive Nutzererfahrung wurden, wo möglich, interaktive Elemente, wie ein Slider, einstellbare Diagramm-Ansichten sowie anklickbare Karten verwendet.Als Hosting-Anbieter haben wir uns für GitHub entschieden, da dies eine kostenlose und sichere Möglichkeit bietet unsere Webseite öffentlich zu präsentieren. Auch das Repository mit verwendeten Code-Dateien und Datensätzen ist so direkt mit der Webseite verknüpft.</p>",
    }
];

export default questions;
